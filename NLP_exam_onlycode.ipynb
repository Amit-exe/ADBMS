{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN74p7zgWcUrpkYuFCvXm6E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amit-exe/ADBMS/blob/main/NLP_exam_onlycode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJOvK_72BiH_"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "HQ2OBAQJBlsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 1\n"
      ],
      "metadata": {
        "id": "G-kVZGI14i9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize , sent_tokenize"
      ],
      "metadata": {
        "id": "qy-UoH_24ne9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"orem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in\""
      ],
      "metadata": {
        "id": "4QjjRSDk4wWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sent_tokenize(dataset)\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "Gfl-3JqM4wTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sen in tokens:\n",
        "  print(sen)"
      ],
      "metadata": {
        "id": "2pJM4OVy4wP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word tokenizer\n",
        "tokens = word_tokenize(dataset)\n",
        "print(tokens)\n",
        "for word in tokens:\n",
        "  print(word)"
      ],
      "metadata": {
        "id": "pLZtBsgW4wNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_data = \"छपाई और अक्षर योजन उद्योग का एक साधारण डमी पाठ है. Lorem Ipsum सन १५०० के बाद से अभी तक इस उद्योग का मानक डमी पाठ मन गया, जब एक अज्ञात मुद्रक ने नमूना लेकर एक नमूना किताब बनाई. यह न केवल पाँच सदियों से जीवित रहा बल्कि इसने इलेक्ट्रॉनिक मीडिया में छलांग लगाने के बाद भी मूलतः अपरिवर्तित रहा. यह 1960 के दशक में Letraset Lorem Ipsum अंश युक्त पत्र के रिलीज के साथ लोकप्रिय हुआ, और हाल ही में Aldus PageMaker Lorem Ipsum के संस्करणों सहित तरह डेस्कटॉप प्रकाशन सॉफ्टवेयर के साथ अधिक प्रचलित हुआ\""
      ],
      "metadata": {
        "id": "Pj4M2qOw4wLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(hindi_data))\n",
        "print(sent_tokenize(hindi_data))"
      ],
      "metadata": {
        "id": "c7v9DB0I4wJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(dataset))"
      ],
      "metadata": {
        "id": "dMeH5iB54wFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "print(tokenizer.tokenize(dataset))"
      ],
      "metadata": {
        "id": "Fu5BL21Q4wCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.tokenize(hindi_data))"
      ],
      "metadata": {
        "id": "hb-pC6Ox4wAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 2"
      ],
      "metadata": {
        "id": "x98ZmVCc694L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "metadata": {
        "id": "HImYMdb069Xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"Three years into the pandemic, China's zero-tolerance measures, from shut borders to frequent lockdowns, contrast sharply with the rest of the world, which has largely decided to live with the virus. The strict approach has battered the world's second-largest economy, put mental strain on hundreds of millions and last month prompted the biggest show of public discontent in mainland China since President Xi Jinping took\""
      ],
      "metadata": {
        "id": "-7w4_F1u4v-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "CB40RZA94v79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(dataset)"
      ],
      "metadata": {
        "id": "eoa2HRJh4v5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "id": "EROfn54r4v3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total words in dataset\",len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZPLgi0N4v0w",
        "outputId": "1a2552d0-899d-488a-bd19-b9697bd4bc4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words in dataset 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "Wgfurscy7lM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopw)\n",
        "print(\"total stop words in corpus\",len(stopw))"
      ],
      "metadata": {
        "id": "MDAUH7AQ7lJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_words=[]\n",
        "stopw_in_dataset = []"
      ],
      "metadata": {
        "id": "8WqXE_PE7lHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  if word in stopw:\n",
        "    stopw_in_dataset.append(word)\n",
        "  else:\n",
        "    filtered_words.append(word)"
      ],
      "metadata": {
        "id": "5BukiCId7lE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_words,\"\\ntotal no of words after removing stop words is \",len(filtered_words) )"
      ],
      "metadata": {
        "id": "qL5x0yjX7lCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'stops words found in data set {stopw_in_dataset} \\n total number of stop words found in dataset {len(stopw_in_dataset)}')"
      ],
      "metadata": {
        "id": "gJMCjP1Y7lAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 3"
      ],
      "metadata": {
        "id": "VrmQX-aK9G3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "d6pSzMkk7k-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"Three years into the pandemic, China's zero-tolerance measures, from shut borders to frequent lockdowns, contrast sharply with the rest of the world, which has largely decided to live with the virus. The strict approach has battered the world's second-largest economy, put mental strain on hundreds of millions and last month prompted the biggest show of public discontent in mainland China since President Xi Jinping took\""
      ],
      "metadata": {
        "id": "m7hhmHP-9I3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(dataset)"
      ],
      "metadata": {
        "id": "qAPzYqkn9Iz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "x3fDfYgx9Ix4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word ,':',ps.stem(word))"
      ],
      "metadata": {
        "id": "CtYxniYg9Ivt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['programmer','programming','program','pythonic','running', 'caring']"
      ],
      "metadata": {
        "id": "T3c38fKJ9ItZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\":\",ps.stem(word))"
      ],
      "metadata": {
        "id": "_cT0FoqH9Iq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LancasterStemmer()"
      ],
      "metadata": {
        "id": "S2sIM7bq9Ion"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\":\",ln.stem(word))"
      ],
      "metadata": {
        "id": "X5VoxhbZ7k7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "q4GGyx0I7k5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word,\":\",wnl.lemmatize(word))"
      ],
      "metadata": {
        "id": "TKHg7TSm-iT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 4\n"
      ],
      "metadata": {
        "id": "u4UxXoxZ_C44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "6NuO69Po-iQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"Three years into the pandemic, China's zero-tolerance measures, from shut borders to frequent lockdowns, contrast sharply with the rest of the world, which has largely decided to live with the virus. The strict approach has battered the world's second-largest economy, put mental strain on hundreds of millions and last month prompted the biggest show of public discontent in mainland China since President Xi Jinping took\""
      ],
      "metadata": {
        "id": "2Dja4WWN-iI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(dataset)"
      ],
      "metadata": {
        "id": "zQ3fB-4N-iFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tag(words)"
      ],
      "metadata": {
        "id": "CXCO1GAu-hsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "PF4Yxfyx-hqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 5\n"
      ],
      "metadata": {
        "id": "Jm1BQlqD_vBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk.chunk import RegexpParser"
      ],
      "metadata": {
        "id": "ERYXELKO-gvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = \"Three years into the pandemic, China's zero-tolerance measures, from shut borders to frequent lockdowns, contrast sharply with the rest of the world, which has largely decided to live with the virus. The strict approach has battered the world's second-largest economy, put mental strain on hundreds of millions and last month prompted the biggest show of public discontent in mainland China since President Xi Jinping took\""
      ],
      "metadata": {
        "id": "zy1isddK_sLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(dataset)"
      ],
      "metadata": {
        "id": "-Mc4uLZU_sHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "id": "HSD4DlQL_sFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fwQNWb5_sCx",
        "outputId": "e7580e1a-ba08-49c0-da62-910a8ab4976a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged = pos_tag(words)"
      ],
      "metadata": {
        "id": "WXCRdegk_sAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_tagged)"
      ],
      "metadata": {
        "id": "kkUIOS9F_r9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = ne_chunk(pos_tagged)"
      ],
      "metadata": {
        "id": "tQmdWisk_r7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunk)"
      ],
      "metadata": {
        "id": "TdG4G0iC_r5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = \"\"\" chunk:{<NNPS>+}\n",
        "{<NNP>+}\n",
        "{<NN>+}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "72gU1zSm_r3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = RegexpParser(pattern)"
      ],
      "metadata": {
        "id": "urexc3i6_r1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cr=chunk.parse(pos_tagged)"
      ],
      "metadata": {
        "id": "ErepF6r0_ry3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cr)"
      ],
      "metadata": {
        "id": "_X9ixJKv_rwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 6"
      ],
      "metadata": {
        "id": "OMhTSoyoC3li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "DL253cBb_ruf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syn = wordnet.synsets('program')"
      ],
      "metadata": {
        "id": "QdSBcxDvC3KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn)"
      ],
      "metadata": {
        "id": "pmRM8lx6C3Gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0])"
      ],
      "metadata": {
        "id": "2ZipN2TVC3DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0].lemmas())"
      ],
      "metadata": {
        "id": "0-8EBvKJC3As"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0].lemmas()[0])"
      ],
      "metadata": {
        "id": "RrnKdidrC292"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0].lemmas()[0].name())"
      ],
      "metadata": {
        "id": "UB1w4_maC28D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0].definition())"
      ],
      "metadata": {
        "id": "61YmgGZgC253"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(syn[0].examples())"
      ],
      "metadata": {
        "id": "2CZ2UA8iC23j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ToFindAntonms(x):\n",
        "  antonym = []\n",
        "  for syn in wordnet.synsets(x):\n",
        "    for lm in syn.lemmas():\n",
        "      if lm.antonyms():\n",
        "        antonym.append(lm.antonyms()[0].name())\n",
        "  return antonym"
      ],
      "metadata": {
        "id": "kTiyF3AfC21U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(ToFindAntonms(\"good\")))"
      ],
      "metadata": {
        "id": "gzrpYO_uC2y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(ToFindAntonms('inactive')))"
      ],
      "metadata": {
        "id": "ULyk6bvyC2vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms =[]\n",
        "antonyms =[]"
      ],
      "metadata": {
        "id": "w1vJEOyQGTdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for syn in wordnet.synsets('good'):\n",
        "  for lm in syn.lemmas():\n",
        "    synonyms.append(lm.name())\n",
        "    if lm.antonyms():\n",
        "      antonyms.append(lm.antonyms()[0].name())"
      ],
      "metadata": {
        "id": "mgHdnUb9GTZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(antonyms))"
      ],
      "metadata": {
        "id": "78-bhoinGTXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(synonyms))"
      ],
      "metadata": {
        "id": "5US4odh-GTCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word similarity\n"
      ],
      "metadata": {
        "id": "hS8ExsoYGS-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "lFsig85uId2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "car = wn.synset('car.n.01')"
      ],
      "metadata": {
        "id": "3wq8AmatGS8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automobile = wn.synset('automobile.n.01')"
      ],
      "metadata": {
        "id": "UZdcpn4JGS6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"similarity between car and automobile\",car.path_similarity(automobile))"
      ],
      "metadata": {
        "id": "SZYpXcnuGS2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sprint = wn.synset('sprint.v.01')"
      ],
      "metadata": {
        "id": "PcgBU81BGSz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = wn.synset('run.v.01')"
      ],
      "metadata": {
        "id": "oVn-sSbcC2rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.wup_similarity(sprint))"
      ],
      "metadata": {
        "id": "y3pXw2LvJRjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jump=wn.synset('jump.v.01')\n",
        "leap=wn.synset('leap.v.01')\n",
        "ship =wn.synset('ship.n.01')\n",
        "boat=wn.synset('boat.n.01')\n",
        "sprint=wn.synset('sprint.v.01')\n",
        "print (\"Similarity between jump and leap \",jump.wup_similarity(leap))"
      ],
      "metadata": {
        "id": "euaSu5U6JRfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Similarity between car and automobile\",car.wup_similarity(automobile))\n",
        "print (\"Similarity between ship and boat \",ship.wup_similarity(boat))\n",
        "print (\"Similarity between run and sprint \",jump.wup_similarity(sprint))\n",
        "print (\"Similarity between jump and leap \",jump.wup_similarity(leap))"
      ],
      "metadata": {
        "id": "VyDujlUbJRcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 7"
      ],
      "metadata": {
        "id": "9HXiExG2J4_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import STOPWORDS ,WordCloud\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "pDW5HdU3JRY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = open('movie_list.txt').read().upper()\n",
        "\n"
      ],
      "metadata": {
        "id": "pO4aCOLCJRVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maskArray = np.array(Image.open('video-300x300.jpg'))"
      ],
      "metadata": {
        "id": "rdClTPsgJRSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cloud = WordCloud(height=1200, width=1400,\n",
        "                  mask=maskArray, font_path='LEMONMILK-Medium.otf',\n",
        "                  background_color=\"#000\",\n",
        "                  min_font_size=4,max_font_size=70,\n",
        "                  collocations=True,stopwords=set(STOPWORDS),\n",
        "                  colormap=\"Blues\").generate(dataset)"
      ],
      "metadata": {
        "id": "AnVCwN5JJRQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,10),facecolor=None)\n",
        "plt.imshow(cloud)\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "yGsD_w2MJRG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRAC 8"
      ],
      "metadata": {
        "id": "DEjYzzK5NnUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag"
      ],
      "metadata": {
        "id": "s-1toeLsNnAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = open('/content/President_Speech.txt',encoding='cp1252').read()"
      ],
      "metadata": {
        "id": "Y8wp68GhJRDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def summerization(text):\n",
        "  result = []\n",
        "  for no, sent in enumerate(sent_tokenize(text)):\n",
        "    no_of_tokens = len(word_tokenize(text))\n",
        "    tagged = pos_tag(word_tokenize(text))\n",
        "    no_of_nn = len([word for word,pos in tagged if pos in ['NN',\"NNP\"]])\n",
        "    ners = nltk.ne_chunk(tagged,binary=False)\n",
        "    no_of_ners = len([chunk for chunk in ners if hasattr(chunk,'label')])\n",
        "    score = (no_of_ners + no_of_nn)/float(no_of_tokens)\n",
        "    result.append((no,score,sent))\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "60n6jJTsN9iX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum = summerization(dataset)"
      ],
      "metadata": {
        "id": "wemokV4fN9e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum"
      ],
      "metadata": {
        "id": "PE1SvojsN9cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sorted(sum,key= lambda x : x[1],reverse=True):\n",
        "  print(i[2])"
      ],
      "metadata": {
        "id": "Z90ZAI6MN9aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_fun(text):\n",
        "  res=[]\n",
        "  for no,sent in enumerate(sent_tokenize(text)):\n",
        "    no_of_tokens=len(word_tokenize(text))\n",
        "    tags = nltk.pos_tag(word_tokenize(text))\n",
        "    no_of_nn = len([word for word,pos in tags if pos in ['NN','NNP']])\n",
        "    ners = nltk.ne_chunk(tags,binary=False)\n",
        "    no_of_ners = len([chunk for chunk in ners if hasattr(chunk,'label')])\n",
        "    score = (no_of_ners+no_of_nn)/float(no_of_tokens)\n",
        "    res.append((no,score,sent))\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "dO1LaCGBN9YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prac 10\n"
      ],
      "metadata": {
        "id": "YZKoU8xBXSJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.corpus import movie_reviews as mr"
      ],
      "metadata": {
        "id": "VatdruLJUnwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = []\n"
      ],
      "metadata": {
        "id": "Ni7Igx0cUzCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ct in mr.categories():\n",
        "  for fid in mr.fileids(ct):\n",
        "    document.append((list(mr.words(fid)),ct))"
      ],
      "metadata": {
        "id": "ikM_Gif0U7NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(document)"
      ],
      "metadata": {
        "id": "Txm_c-_lWMvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document[0])"
      ],
      "metadata": {
        "id": "jaOb52hgVidM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_w=[]\n",
        "for w in mr.words():\n",
        "  all_w.append(w.lower())"
      ],
      "metadata": {
        "id": "jgKy4nU6Vlmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = nltk.FreqDist(all_w)\n",
        "print(fdist.most_common(15))\n",
        "print(fdist['love'])"
      ],
      "metadata": {
        "id": "S0UUmUjrWjvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_features = list(fdist.keys())[:3000]"
      ],
      "metadata": {
        "id": "xvDOTCM6W7Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_features(word_list):\n",
        "  words=set(word_list)\n",
        "  features={}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words);\n",
        "  return features"
      ],
      "metadata": {
        "id": "EGxtkf5OXRLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_features(mr.words('neg/cv000_29416.txt'))"
      ],
      "metadata": {
        "id": "gJJjiOu9X08X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(find_features(rev), category) for (rev, category) in document]"
      ],
      "metadata": {
        "id": "iFz07s07X7MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = featuresets[:1900]\n",
        "testing_set = featuresets[1900:]"
      ],
      "metadata": {
        "id": "Q4ohcGHqbKiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(training_set)"
      ],
      "metadata": {
        "id": "h62CwCa2bWAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", (nltk.classify.accuracy(classifier, testing_set))*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7JTATWmbhNe",
        "outputId": "8b2a0d17-8c08-48b2-df10-492456f81b46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(15)"
      ],
      "metadata": {
        "id": "kgLXO0cLbqE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prac 9"
      ],
      "metadata": {
        "id": "XH_H5CYpXetf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "T87u4x68djDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string \n",
        "import warnings\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "8gc21SfCdnt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(action=\"ignore\", category = UserWarning, module=\"gensim\")"
      ],
      "metadata": {
        "id": "D4xvt4xXeHh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from wikipedia import search,page"
      ],
      "metadata": {
        "id": "AyYFHfhGeNEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "titles = search('data science')"
      ],
      "metadata": {
        "id": "VUzImSnheWal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipage = page(titles[0])"
      ],
      "metadata": {
        "id": "JFyPWAtreblh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipage.content"
      ],
      "metadata": {
        "id": "6NSMiUlUeclf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipage.categories"
      ],
      "metadata": {
        "id": "2iCBOXtme1gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikipedia.summary(wikipage, sentences=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qDgdzxdSe6Dd",
        "outputId": "cc9cf2d8-731a-46f0-c426-d831dece6daa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This page provides supplementary data to the article properties of water.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search('data science')"
      ],
      "metadata": {
        "id": "-ODqqjaFfAu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "  result=[]\n",
        "  sent = sent_tokenize(text)\n",
        "  for sentence in sent:\n",
        "    stopw = stopwords.words('english')\n",
        "    words = word_tokenize(sentence)\n",
        "    lemma = WordNetLemmatizer()\n",
        "    tokens = [w for w in words if w.lower() not in string.punctuation]\n",
        "    tokens = [w for w in tokens if w not in stopw]\n",
        "    tokens = [w for w in tokens if len(w)>=3]\n",
        "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
        "    result +=[tokens]\n",
        "\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "qWwO1RFafSCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_p = preprocessing(wikipage.content)"
      ],
      "metadata": {
        "id": "mCwpDNE_hsml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_p"
      ],
      "metadata": {
        "id": "QvA-IZrYiP5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_count=2\n",
        "size=50\n",
        "window = 4"
      ],
      "metadata": {
        "id": "wzQJyPO0iZ2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikimodel = Word2Vec(text_p,min_count=min_count,size=size, window=window)"
      ],
      "metadata": {
        "id": "pH0nfbMki8bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(wikimodel.wv.vocab.keys())"
      ],
      "metadata": {
        "id": "pEM_gcjOjZOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "DMYu-Z77jh_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wikimodel.wv.most_similar(positive = [\"learning\", \"launched\"], topn = 3)"
      ],
      "metadata": {
        "id": "Q07XtSNmji9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wikimodel.wv.similarity(\"learning\", \"help\"))"
      ],
      "metadata": {
        "id": "rDOIiE9Zj3aM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(wikimodel.wv.similarity(\"first\", \"three\"))"
      ],
      "metadata": {
        "id": "jvfDoOUEkRSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IxrsLkZtWXBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}